{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of StudentCopy_Section_2_FakeNews.ipynb","provenance":[{"file_id":"1odCS7JpbXu_eaR7VQXnmcmJi35I5B4AJ","timestamp":1625328734469}],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gVgIxW-YPeQH"},"source":["# Section 2: Creating a Baseline Model for Fake News Classification\n"]},{"cell_type":"markdown","metadata":{"id":"p6yXc9Z2KMGy"},"source":["In this notebook we'll be:\n","1.   Performing feature Selection\n","2.   Understanding Performance Metrics\n","\n"]},{"cell_type":"code","metadata":{"id":"qT8bCpXGr2nO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625328800531,"user_tz":-330,"elapsed":7592,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"1d9aa745-4626-499b-843d-44b51b046cdc"},"source":["#@title Run this code to get started\n","import math\n","import os\n","import numpy as np\n","\n","import pickle\n","\n","import requests, io, zipfile\n","\n","# Download class resources...\n","!wget -O data.zip 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Fake%20News%20Detection/inspirit_fake_news_resources%20(1).zip'\n","!unzip data.zip\n","\n","basepath = '.'\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2021-07-03 16:13:12--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Fake%20News%20Detection/inspirit_fake_news_resources%20(1).zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.128, 64.233.189.128, 108.177.97.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 109422100 (104M) [application/zip]\n","Saving to: ‘data.zip’\n","\n","data.zip            100%[===================>] 104.35M   181MB/s    in 0.6s    \n","\n","2021-07-03 16:13:13 (181 MB/s) - ‘data.zip’ saved [109422100/109422100]\n","\n","Archive:  data.zip\n","  inflating: train_val_data.pkl      \n","  inflating: test_data.pkl           \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8x4L6VvhR6wp"},"source":["## Exercise 1\n","\n","Below we introduce some code for taking our training and val data and producing X, y examples that can be fit by a logistic regression model. This code extracts a few basic features from the domain name extension of the website. **Your task: add features testing whether the domain name ends in \".co\", \".tv\", and \".news\", according to the template below.**\n"]},{"cell_type":"code","metadata":{"id":"VgG3BG3mh8r_","executionInfo":{"status":"ok","timestamp":1625328977194,"user_tz":-330,"elapsed":735,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}}},"source":["def prepare_data(data, featurizer):\n","    X = []\n","    y = []\n","    for datapoint in data:\n","        url, html, label = datapoint\n","        # We convert all text in HTML to lowercase, so <p>Hello.</p> is mapped to\n","        # <p>hello</p>. This will help us later when we extract features from \n","        # the HTML, as we will be able to rely on the HTML being lowercase.\n","        html = html.lower() \n","        y.append(label)\n","\n","        features = featurizer(url, html)\n","\n","        # Gets the keys of the dictionary as descriptions, gets the values\n","        # as the numerical features. Don't worry about exactly what zip does!\n","        feature_descriptions, feature_values = zip(*features.items())\n","\n","        X.append(feature_values)\n","\n","    return X, y, feature_descriptions\n","  \n","# Returns a dictionary mapping from plaintext feature descriptions to numerical\n","# features for a (url, html) pair.\n","def domain_featurizer(url, html):\n","    features = {}\n","    \n","    # Binary features for the domain name extension.\n","    features['.com domain'] = url.endswith('.com')\n","    features['.org domain'] = url.endswith('.org')\n","    features['.net domain'] = url.endswith('.net')\n","    features['.info domain'] = url.endswith('.info')\n","    features['.org domain'] = url.endswith('.org')\n","    features['.biz domain'] = url.endswith('.biz')\n","    features['.ru domain'] = url.endswith('.ru')\n","    features['.co.uk domain'] = url.endswith('.co.uk')\n","    \n","    ### YOUR CODE HERE ###\n","    features['.co domain'] = url.endswith('.co')\n","    features['.tv domain'] = url.endswith('.tv')\n","    features['.news domain'] = url.endswith('.news')\n","    ### END CODE HERE ###\n","    \n","    return features"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RncfRR2uzNxg"},"source":["## Instructor-Led Discussion: Deciding Inputs to our Model\n","\n","Make sure you understand what the code above is doing. It produces X, y such that X contains a list of features for each site in the dataset, and y contains the labels in corresponding order. *feature_descriptions* is a list of the names of features (.e.g., '.com domain'). This will be important later when we want to know the names of features when interpreting the model. Let's run our code for processing the data on the train and val sets from yesterday.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8zgoSqkV97WY"},"source":["## Exercise 2"]},{"cell_type":"markdown","metadata":{"id":"HdPBd5429-Sk"},"source":["\n","Your task: call *prepare_data* twice, once on *train_data* and *domain_featurizer* and once on *val_data* and *domain_featurizer*. Save the results as *train_X, train_y, feature_descriptions* and *val_X, val_y, feature_descriptions*."]},{"cell_type":"code","metadata":{"id":"DN6_-ViTeCYF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625329248773,"user_tz":-330,"elapsed":6175,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"b3f34179-3b61-42bf-8079-69a32b5b102e"},"source":["with open(os.path.join(basepath, 'train_val_data.pkl'), 'rb') as f:\n","  train_data, val_data = pickle.load(f)\n","  \n","print('Number of train examples:', len(train_data))\n","print('Number of val examples:', len(val_data))\n","  \n","### YOUR CODE HERE ###\n","x_train, y_train, feature_descriptions = prepare_data(train_data, domain_featurizer)\n","val_x, val_y, feature_descriptions = prepare_data(val_data, domain_featurizer) \n","### END CODE HERE ###\n","\n","print('Number of features per example:', len(x_train[0]))\n","print('Feature descriptions:')\n","print(feature_descriptions)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Number of train examples: 2002\n","Number of val examples: 309\n","Number of features per example: 10\n","Feature descriptions:\n","('.com domain', '.org domain', '.net domain', '.info domain', '.biz domain', '.ru domain', '.co.uk domain', '.co domain', '.tv domain', '.news domain')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vSeQLoyT-Avf"},"source":["Now to train on our featurized data. We use scikit-learn as in the previous week, because it makes it easy to quickly iterate on different types of models.\n"]},{"cell_type":"markdown","metadata":{"id":"MA0H-ezT3A3-"},"source":["\n","## Exercise 3\n"]},{"cell_type":"markdown","metadata":{"id":"78LVe7ql-DSv"},"source":["Another quick exercise: load the LogisticRegression model from scikit-learn with default parameters (no arguments to the constructor). Then fit it on *train_X* and *train_y* (~5 minutes). "]},{"cell_type":"code","metadata":{"id":"U2Xqs6b0eTgQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625329794066,"user_tz":-330,"elapsed":479,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"4f672eb0-59bc-44eb-cae4-7d94e72310e3"},"source":["### YOUR CODE HERE ###\n","from sklearn import linear_model\n","\n","baseline_model = linear_model.LogisticRegression()\n","baseline_model.fit(x_train,y_train)\n","\n","### END CODE HERE ###"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"WchEBxFeA46-"},"source":["## Exercise 4 \n","\n"]},{"cell_type":"code","metadata":{"id":"xv_PHkE6346w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625329850144,"user_tz":-330,"elapsed":542,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"275238be-a7d3-42ae-b1a4-c68238962696"},"source":["train_y_pred = baseline_model.predict(x_train)\n","print('Train accuracy', accuracy_score(y_train, train_y_pred))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Train accuracy 0.6108891108891109\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ylD5OWTZoCz2"},"source":["We can see that we are not doing very well, but we are doing better than 50%. We can do the same for the val data to see how we are doing on unseen data, which is more valuable for us if we want to make predictions on new websites. Fill in the code below to evaluate val accuracy!"]},{"cell_type":"code","metadata":{"id":"MUsF1H8uiN2g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625330078246,"user_tz":-330,"elapsed":513,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"72bd8ef9-45fc-46a4-ae7b-393e79170c4c"},"source":["### YOUR CODE HERE ###\n","val_y_pred = baseline_model.predict(val_x)\n","\n","### END CODE HERE ###\n","print('Val accuracy', accuracy_score(val_y, val_y_pred))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Val accuracy 0.5533980582524272\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hq6sGGVHobjo"},"source":["We appear to be doing a bit worse on the val data, not much better than chance. To better understand the performance of our binary classification model, we should seek to better understand the mistakes that it is making. Specifically, when our model makes a mistake (about 40% of the time), are these mistakes false negatives or false positives?"]},{"cell_type":"markdown","metadata":{"id":"0A_HLeiz9IlH"},"source":["## Confusion Matrix "]},{"cell_type":"markdown","metadata":{"id":"VeWbCvMa8rhg"},"source":["To answer these questions, we produce and analyze the confusion matrix. The confusion matrix is a matrix that shows the following:\n","\n","![Confusion Matrix](https://cdn-images-1.medium.com/max/1600/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n","\n","where the terms mean\n","\n","* TP (True Positive) = You predicted positive (fake in our case, since fake has a label of 1) and it’s true.\n","* FP (False Positive) = You predicted positive and it’s false.\n","* FN (False Negative) = You predicted negative and it’s false.\n","* TN (True Negative) = You predicted negative and it’s true.\n","\n","\n","###Common Metrics\n","\n","From the confusion matrix, we can extract commonly used metrics like precision (TP/(TP + FP)) and recall (TP/(TP + FN)). \n","\n","* Precision quantifies how often the things we classify as positive are actually positive. For our task, this measures what fraction of the sites we classify as fake are actually fake. \n","* Recall quantifies what fraction of actually positive examples we classify as positive. In our case, this is the fraction of fake news websites that we actually identify as fake.\n","\n","Finally, a useful score to summarize both precision and recall is the F-1 score. This is just a simple function (the harmonic mean) of precision and recall, shown in the summary below:\n","\n","<img src=\"https://datascience103579984.files.wordpress.com/2019/04/capture3-24.png\" width=\"400\" height=\"200\"></img>"]},{"cell_type":"markdown","metadata":{"id":"pftTwsCvE8Lt"},"source":["##Exercise 5 |  Using the Confusion Matrix "]},{"cell_type":"markdown","metadata":{"id":"tc9BxsQ-GYOo"},"source":["Run the cell below to create the confusion matrix for our own model. "]},{"cell_type":"code","metadata":{"id":"M9aYm4oqsL6P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625330502113,"user_tz":-330,"elapsed":532,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"6d9fc4ad-3b52-4801-8dc6-865f32cfbf48"},"source":["print('Confusion matrix:')\n","print(confusion_matrix(val_y, val_y_pred))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Confusion matrix:\n","[[142  26]\n"," [112  29]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q5Of0ZHsHZ0C"},"source":["A Confusion Matrix can quickly tell you how well your model is doing. The primary way to figure this out is to calculate the Error Rate. \n","\n","The Error Rate is:   (FP) + (FN)) / (TP + FP + FN + TN).\n","\n","This is just all the false predictions (False Negative + False Positive) divided by all the predictions added together.  \n","\n","Use the Confusion Matrix we just created to calculate the Error Rate for our model. "]},{"cell_type":"code","metadata":{"id":"Bo0Bej9z_xcq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625330888252,"user_tz":-330,"elapsed":498,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"9195f923-e5f0-4021-a208-c8ca850c9af0"},"source":["### YOUR CODE HERE ###\n","t,b = confusion_matrix(val_y, val_y_pred)\n","tp = t[0]\n","fp = t[1]\n","fn = b[0]\n","tn = b[1]\n","\n","error = (fp + fn)/(tp + fp + fn + tn)\n","error\n","### END CODE HERE ###"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.44660194174757284"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"1ac9XXM9XsU0"},"source":["## Exercise 6"]},{"cell_type":"code","metadata":{"id":"gVxJhwhzspF-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625330911030,"user_tz":-330,"elapsed":481,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"45527c37-e123-44fb-bf58-3aa54a4868d5"},"source":["print(val_y_pred)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n"," 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0\n"," 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0\n"," 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n"," 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0\n"," 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0\n"," 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n"," 0 0 0 0 0 1 0 0 0 0 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nh-qi0tvGvFH"},"source":["We can see that we have many false negatives, and not as many false positives. Why is this the case? If we print out *val_y_pred*, we can see that our model is mostly predicting 0's (websites are real)."]},{"cell_type":"markdown","metadata":{"id":"fa5GCh7eG7OU"},"source":["What fraction of predictions in *val_y_pred* are 1's? Hint: you may find *np.mean* useful."]},{"cell_type":"code","metadata":{"id":"pStoI4cLfKDv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625331120239,"user_tz":-330,"elapsed":476,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"ffcf7e22-a830-4808-d7b9-dfc0154850ad"},"source":["### YOUR CODE HERE ###\n","num = 0\n","for i in range(len(val_y_pred)):\n","  if val_y_pred[i] == 1:\n","    num = num + 1\n","\n","print(num/len(val_y_pred))\n","print(np.mean(val_y_pred))\n","### END CODE HERE ###"],"execution_count":22,"outputs":[{"output_type":"stream","text":["0.1779935275080906\n","0.1779935275080906\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eQdhOeigtmCp"},"source":["Why so many 0's? The only information we are giving our model is its domain name extension. It's natural that the model would learn that websites with \".biz\" extensions are unlikely to be reliable news websites, but it is still the case that most websites in the dataset (fake and real) have \".com\" extensions. Thus, our model will misclassify many fake news websites with \".com\" extensions as real. "]},{"cell_type":"code","metadata":{"id":"tHQTv6L98uno","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625331252982,"user_tz":-330,"elapsed":483,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"ffc62ee8-c2a6-487f-c8c2-f155ab68d4b9"},"source":["prf = precision_recall_fscore_support(val_y, val_y_pred)\n","\n","print('Precision:', prf[0][1])\n","print('Recall:', prf[1][1])\n","print('F-Score:', prf[2][1])"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Precision: 0.5272727272727272\n","Recall: 0.20567375886524822\n","F-Score: 0.29591836734693877\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vw9xDPTsruaB"},"source":["Again, the precision and recall metrics suggest that when we classify a website as fake, we are usually right, but we are not doing great at classifying these websites as fake frequently enough."]},{"cell_type":"markdown","metadata":{"id":"vyJC74B7ALAy"},"source":["##Using Keywords for a Stronger Baseline "]},{"cell_type":"markdown","metadata":{"id":"_3lY7pMkAPpX"},"source":["The key problem with our model in its current state is that it simply does not have enough information. This should not be a surprise–it was pretty unlikely in the first place that domain name extensions would be enough. If you like, feel free to add a few more extensions in the “featurizer” above and re-run all the code for evaluation–you'll find it doesn't make much of a difference.\n","Where can we get more information about webpages? From the HTML! Remember that the HTML contains all of the text and structure of a webpage. If we cleverly choose features from the HTML to feed into our logistic regression model, we will drastically improve our performance. We saw yesterday that probing hypotheses related to the counts of hypotheses words produced interesting results, and we will continue in this direction today to produce a model that leverages these differences in word frequencies.\n"]},{"cell_type":"markdown","metadata":{"id":"TyRp6ooJAnng"},"source":["## Exercise 7: Instructor-Led Discussion on Better Input Features\n"]},{"cell_type":"markdown","metadata":{"id":"L4jjZ5HF35K0"},"source":["\n","The below code introduces a better featurizer that counts the number of keywords (normalized using the *log* function) in the HTML. Normalizing the counts is a trick that prevents the featurized values from becoming too extreme. Read the code and make sure you understand what it is doing. Then add \"sports\" and \"finance\" as additional keywords to expand our model.\n","\n","**Run the below code and discuss what it is doing as a class. Add in additional keywords to further expand our model as you see fit.**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"eeyWAqI9idZ0","executionInfo":{"status":"ok","timestamp":1625333125695,"user_tz":-330,"elapsed":510,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}}},"source":["# Gets the log count of a phrase/keyword in HTML (transforming the phrase/keyword\n","# to lowercase).\n","def get_normalized_count(html, phrase):\n","    return math.log(1 + html.count(phrase.lower()))\n","\n","\n","# Returns a dictionary mapping from plaintext feature descriptions to numerical\n","# features for a (url, html) pair.\n","def keyword_featurizer(url, html):\n","    features = {}\n","    \n","    # Same as before.\n","    features['.com domain'] = url.endswith('.com')\n","    features['.org domain'] = url.endswith('.org')\n","    features['.net domain'] = url.endswith('.net')\n","    features['.info domain'] = url.endswith('.info')\n","    features['.org domain'] = url.endswith('.org')\n","    features['.biz domain'] = url.endswith('.biz')\n","    features['.ru domain'] = url.endswith('.ru')\n","    features['.co.uk domain'] = url.endswith('.co.uk')\n","    features['.co domain'] = url.endswith('.co')\n","    features['.tv domain'] = url.endswith('.tv')\n","    features['.news domain'] = url.endswith('.news')\n","    \n","    ### YOUR CODE HERE ###\n","    keywords = [\"sports\",\"elections\",\"leak\",\"reports\",\"officials\",\"latest\",\"comments\",\"released\"] \n","    ### END CODE HERE\n","\n","\n","    \n","    for keyword in keywords:\n","      features[keyword + ' keyword'] = get_normalized_count(html, keyword)\n","\n","\n","    \n","    return features\n","\n"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sk_BBflEAuX3"},"source":["##Exercise 8"]},{"cell_type":"markdown","metadata":{"id":"bgWx3s7UxDue"},"source":["\n","\n","Let's run and evaluate the above featurizer. Add in code to fit the model, compute train accuracy, val accuracy, val confusion matrix, and val precision, recall, and F1-Score, just as before."]},{"cell_type":"code","metadata":{"id":"fxEIPqn1f1iQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625333137435,"user_tz":-330,"elapsed":8156,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"207f20ff-f0c8-4485-a431-903edbb73a36"},"source":["train_X, train_y, feature_descriptions = prepare_data(train_data, keyword_featurizer)\n","val_X, val_y, feature_descriptions = prepare_data(val_data, keyword_featurizer)\n","\n","print('Number of features per example:', len(train_X[0]))\n","print('Feature descriptions:')\n","print(feature_descriptions)\n","print()\n","  \n","baseline_model = LogisticRegression()\n","\n","### YOUR CODE HERE ###\n","baseline_model.fit(train_X, train_y)\n","train_y_pred = baseline_model.predict(train_X)\n","print('Train accuracy score: ', accuracy_score(train_y, train_y_pred))\n","val_y_pred = baseline_model.predict(val_X)\n","print('Val Accuracy score : ' + str( accuracy_score(val_y, val_y_pred)))\n","print('Confusion matrix : ')\n","print(confusion_matrix(val_y, val_y_pred))\n","\n","prf = precision_recall_fscore_support(val_y, val_y_pred)\n","\n","print('Precision:', prf[0][1])\n","print('Recall:', prf[1][1])\n","print('F-Score:', prf[2][1])\n","### END CODE HERE ###"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Number of features per example: 18\n","Feature descriptions:\n","('.com domain', '.org domain', '.net domain', '.info domain', '.biz domain', '.ru domain', '.co.uk domain', '.co domain', '.tv domain', '.news domain', 'sports keyword', 'elections keyword', 'leak keyword', 'reports keyword', 'officials keyword', 'latest keyword', 'comments keyword', 'released keyword')\n","\n","Train accuracy score:  0.8271728271728271\n","Val Accuracy score : 0.8187702265372169\n","Confusion matrix : \n","[[122  46]\n"," [ 10 131]]\n","Precision: 0.7401129943502824\n","Recall: 0.9290780141843972\n","F-Score: 0.8238993710691823\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1l9Jq6CzwWOt"},"source":["## Interpreting our Model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_hTNITBgB871"},"source":["### Instructor-Led Discussion: Interpreting Input Variables"]},{"cell_type":"markdown","metadata":{"id":"43z3A67xB4Qu"},"source":["As mentioned earlier, a key motivation for using a simpler model is interpretability.\n","\n","We've learned that the prediction of a logistic regression classifier is just the output of a multiplication with model weights, followed by a non-linear transformation (sigmoid). Because the sigmoid function is always increasing (monotonic) on its domain (see below), we know that if the dot product (or multiplication of vectors) between model weights and input features is large, then the output prediction will be closer to 1. If the dot product is small, then the output prediction will be closer to 0.\n","\n","![Sigmoid](https://cdn-images-1.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png)\n","\n","Thus, the weights corresponding to features tell us whether the features are important in the classification. If the weight corresponding to the feature \".net domain\" has a large positive value, then websites with \".net\" domains are more likely to be classified as fake (since fake has label 1). If it has a large negative value, then these websites are more likely to be classified as real. If it has value close to 0, then the feature may not be useful (at least, it may not be useful given that the other features are present).\n"]},{"cell_type":"markdown","metadata":{"id":"nqIKnF6fCgnP"},"source":["###Using Feature Descriptions"]},{"cell_type":"markdown","metadata":{"id":"0G8LwtG2Cj9u"},"source":["Let's see what weights our model learned. The code below uses *feature_descriptions* and the weights, or coefficients, of the model and sorts them in ascending order."]},{"cell_type":"code","metadata":{"id":"QHcJkqd4zic8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625332295723,"user_tz":-330,"elapsed":508,"user":{"displayName":"Arhan Singhal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4PdbuZAsAVBem9gfXxGllkTugQRSsMItz_luU=s64","userId":"10909470486853968911"}},"outputId":"63c83a78-9f79-4d5a-8f64-ca1178057b08"},"source":["sorted(zip(feature_descriptions, baseline_model.coef_[0].tolist()), key=lambda x: x[1])"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('sports keyword', -1.073043544756534),\n"," ('.com domain', -0.07531165936527803),\n"," ('hillary keyword', 0.17924964503849397),\n"," ('.biz domain', 0.2719313403688504),\n"," ('.org domain', 0.48556524635201453),\n"," ('obama keyword', 0.6326444619936858),\n"," ('.ru domain', 1.0428368358203826),\n"," ('.news domain', 1.0428368358203826),\n"," ('.tv domain', 1.1849477227984482),\n"," ('.info domain', 1.3766721648093891),\n"," ('.co.uk domain', 1.7481319666252089),\n"," ('.net domain', 2.512189155355654),\n"," ('.co domain', 2.7147060251074286)]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"qfIAQeg5zpRL"},"source":["## Exercise 9\n","\n","Answer the following questions:\n","\n","* What features have positive weight (most predictive of being fake)? What does that indicate?\n","* Which ones have negative weight (most predictive of being real)? What does that indicate?\n","* Which ones have close to 0 weight? \n","* Are there any feature weights that surprise you? \n","* Try coming up with explanations for why the feature weights are the way they are. Does this help you come up with new feature ideas? (~15 minutes)"]},{"cell_type":"code","metadata":{"id":"gnYk7Vp_YiSO"},"source":["'''\n","1. .co, .net Positive weight means the programs feels that the particular feature contributes towards categorizing the website as fake.\n","2.  \n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"082Dut9agst6"},"source":["## Instructor-Led Discussion: Final Interpretation of Inputs"]},{"cell_type":"markdown","metadata":{"id":"gfQvDlNAYjAa"},"source":["##Exercise 10 |  Final Baseline\n","\n","Finally, play around with the last few cells, adding more keywords and domain names to see how the results change. Note that \"keywords\" can be a variety of things: English words, English phrases (spaces are allowed), HTML tags, and any other string present in HTML. Also notice how the weights on different features vary–you may observe some interesting effects. When you are done, run the cell below to run evaluations again!"]},{"cell_type":"code","metadata":{"id":"19rbiCbuP8iq"},"source":["train_y_pred = baseline_model.predict(train_X)\n","print('Train accuracy', accuracy_score(train_y, train_y_pred))\n","\n","val_y_pred = baseline_model.predict(val_X)\n","print('Val accuracy', accuracy_score(val_y, val_y_pred))\n","\n","print('Confusion matrix:')\n","print(confusion_matrix(val_y, val_y_pred))\n","\n","prf = precision_recall_fscore_support(val_y, val_y_pred)\n","\n","print('Precision:', prf[0][1])\n","print('Recall:', prf[1][1])\n","print('F-Score:', prf[2][1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6aOTjSX0_yj"},"source":["Congratulations on completing this notebook. Looking at the results of our final baseline, you may be surprised this approach is working at all–after all, our model is still barely looking at the content of websites. We will further explore the issue of modeling the content of websites tomorrow, but as a result of our efforts today, we now know that we can make progress with a relatively simple approach!"]}]}